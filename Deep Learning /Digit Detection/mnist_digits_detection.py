# -*- coding: utf-8 -*-
"""mnist_digits_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NuG0-gw6CJSySmz-AyaWFhXTV2cW6khM

# **Simple Neural Network for Handwritten Digit Recognition**

### **Description**
The MNIST dataset consists of 70,000 images of handwritten digits, each of size 28x28 pixels. Our goal is to train a neural network to classify these images into their corresponding digit classes. The project involves the following steps:

- **Data Preparation**: Load and preprocess the MNIST dataset.
- **Model Creation**: Build a simple feedforward neural network.
- **Model Training**: Train the model on the training set.
- **Evaluation**: Evaluate the model's performance on the test set.
- **Prediction**: Use the trained model to make predictions on new data.
"""

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import to_categorical

"""1. **Data Preparation**:
   - We load the MNIST dataset using `mnist.load_data()`, which returns training and test sets.
   - The pixel values are normalized to the range [0, 1] for better convergence during training.
   - Labels are converted to categorical format using `to_categorical()`.
"""

# Step 1: Data Preparation
# Load the MNIST dataset
(xTrain, yTrain), (xTest, yTest) = mnist.load_data()

xTrain, yTrain

print(xTrain, yTrain)

xTest, yTest

print(xTest, yTest)

# Normalize the pixel values to the range [0, 1]
xTrain = xTrain.astype('float32') / 255.0
xTest = xTest.astype('float32') / 255.0

xTrain
xTest

# Convert labels to categorical format
yTrain = to_categorical(yTrain, num_classes=10)
yTest = to_categorical(yTest, num_classes=10)

yTrain, yTest

print(yTrain, yTest)

"""2. **Model Creation**:
   - A sequential model is initialized, and we add layers:
     - **Flatten Layer**: Converts the 28x28 images into a 784-dimensional vector.
     - **Dense Layer**: A hidden layer with 128 neurons and ReLU activation function.
     - **Output Layer**: A dense layer with 10 neurons (for digits 0-9) and softmax activation function to output probabilities.
"""

# Step 2: Model Creation
# Initialize the model
model = Sequential()

# Add layers to the model
model.add(Flatten(input_shape=(28, 28))) # Flatten the input
model.add(Dense(128, activation='relu')) # Hidden layer with 128 neurons
model.add(Dense(10, activation='softmax')) # Output layer with 10 classes

"""3. **Model Compilation**:
   - The model is compiled using the Adam optimizer and categorical cross-entropy loss function, with accuracy as a metric.
"""

# Step 3: Model Compilation
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

"""4. **Model Training**:
   - The model is trained on the training set for 10 epochs with a batch size of 32. We also use 20% of the training data for validation.
"""

# Step 4: Model Training
model.fit(xTrain, yTrain, epochs=10, batch_size=32, validation_split=0.2)

"""5. **Model Evaluation**:
   - The model is evaluated on the test set, and the accuracy is printed.
"""

# Step 5: Model Evaluation
test_loss, test_accuracy = model.evaluate(xTest, yTest)
print(f'Test accuracy: {test_accuracy:.4f}')

"""6. **Prediction**:
   - The model predicts the labels of the test set, and we visualize the first five test images along with their predicted and actual labels.
"""

# Step 6: Prediction
predictions = model.predict(xTest)

# Display the first 5 test images and their predicted labels
for i in range(5):
    plt.imshow(xTest[i], cmap='gray')
    plt.title(f'Predicted: {np.argmax(predictions[i])}, Actual: {np.argmax(yTest[i])}')
    plt.axis('off')
    plt.show()